---
title: 우분투 메모리 이슈 및 해결
date: 2024-05-07
tags:
  - ubuntu
  - low memory
  - buffer memory
  - cache memory
  - pytorch-forecasting
  - drop caches
  - tensorboard
draft: false
summary: "공용 jupyter server를 만들어서 사용하면서 생긴 메모리 부족 현상과 해결"
type: Blog
images: []
---

# 메모리 부족 현상
서버 운영 경험이 전혀 없는 상태에서 공용으로 사용하기 위한 jupyter 서버를 개설하였다. 사용자라고 해봐야 3명 정도인데 문제가 지속적으로 발생하였다. 램 메모리가 지속해서 부족하여 서버가 멈추는 현상이 발생하였다. 두 가지 원인에 의해서 발생한 것으로 보인다. 첫 번째 원인은 python 코드 상의 문제인 것으로 보였고, 또 하나는 buffer / cache memory에 의해서 발생하였다. 원인 파악은 리눅스에서  `top`와 `free -h` 명령어를 사용하여 확인하였다.

## pytorch-forecasting과 관련된 문제
우선 첫 번째 python 코드에 관한 것은 분석에 활용하고 있는 패키지로 인해 발생한 것으로 보인다. `pytorch-forecasting`을 이용하여 시계열 예측 하는 작업을 하고 있었다. 정상적인 상황이라면 epoch를 반복하더라도 점유하고 있는 메모리가 증가하지 않고 유지되어야 하는 것 같다. 원인은 validation 과정에서 발생하는 것 같았다. 
validation 시에 gradients가 계산되지 않도록 설정해야 하는데 이게 제대로 되어있지 않다면 메모리 점유율이 지속해서 증가하는 것으로 보인다. 처음에는 이 부분을 의심했으나, `pytorch-forecasting` 패키지가 `lightning` 기반으로 되어있어서 이 문제가 발생하는 것은 아닌 것으로 생각되었다. 또 한가지 의심해볼만한 것은 매 validation step마다 그래프를 그리는 데 이부분에 의해서 점유율이 증가할 수도 있을 것 같다.
어찌됐든 매 step마다 증가하는 메모리 점유율이 엄청나게 크지는 않아서 메모리를 하나 추가로 더 장착하는 방법으로 해결하였다.

## buffer / cache memory 이슈
메모리를 추가로 장착 했음에도 불구하고 지속적으로 가용 메모리가 작아져 있는 것을 발견하였다. `free -h`로 확인해보니, buffer / cache memory가 전체의 60% 이상을 차지하고 있었다. 해당 부분은 I/O 과정을 빠르게 하거나 리눅스 내에서 반복적인 작업에 대한 부분을 줄이기 위한 것 같다. 그래도 검색해보니 이 부분이 계속 쌓이는 것은 문제가 되는 것 같다. 이 부분이 지나치게 쌓이면 비워주는 작업을 통해서 문제를 해결할 수 있다. 아래 코드에서 `echo` 옆 숫자는 2나 3으로 변경해서 사용할 수 있다.

```sh
sync
echo 1 > /proc/sys/vm/drop_caches
```

해당 작업을 crontab을 이용해서 자동으로 반복하게 할 수도 있다.

## 잠재적인 문제(tensorboard)
현재 텐서 보드도 꽤나 많은 메모리를 차지 하고 있다. 이 부분은 로그가 지속적으로 쌓이면서 발생하는 문제인 것으로 보인다. 텐서 보드에서 표현되어야 하는 로그가 많아질수록 메모리에도 더 많은 데이터가 쌓이면서 점유율이 높아지는 것으로 보인다. 이 부분은 불필요한 실험 데이터를 지우는 방향으로 해결하면 될 것 같다.

# 요약 및 기타 알게 된 사항들
- 메모리 부족 현상 발생
	- `pytorch-forecasting` 패키지 자체에 무언가 문제가 있는 듯 보임
	- buffer / cache memory가 점차 커지는 현상
- 해결방안
	- 메모리를 추가로 장착
	- buffer / cache 메모리를 주기적으로 비워주는 작업 수행
- 기타
	- swap 메모리를 사용하지 않음. RAM의 가격이 낮아진 이유일까?
	- 따라서, RAM 메모리가 부족하면 서버가 잘 못 구축된 것임
- 느낀점
	- 고작 3명이 쓰는 것도 은근 손이 많이 갈 수 있음